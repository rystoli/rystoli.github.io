<!DOCTYPE HTML>
<!--
	Read Only by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Ryan M Stolier | New York University</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1" />
		<!--[if lte IE 8]><script src="assets/js/ie/html5shiv.js"></script><![endif]-->
		<link rel="stylesheet" href="./assets/css/main.css" />
		<!--[if lte IE 8]><link rel="stylesheet" href="assets/css/ie8.css" /><![endif]-->
		<link rel="stylesheet" href="./assets/ai/css/academicons.css"/>
		<link rel="icon" type="image/png" href="./images/favicon.png" sizes="32x32">

	</head>
	<body>

		<!-- Header -->
			<section id="header">
				<header>
					<span class="image avatar"><img src="./images/avatar.jpg" alt="" /></span>
					<h1 id="logo">Ryan M. Stolier</h1>
					
				</header>
				<nav id="nav">
					<ul>
						<li><a href="./index.html" class="active">Back</a></li>
					</ul>
				</nav>
				<footer>
					<ul class="icons">
					<li><font size="6"><a href="./files/CV_Stolier.pdf">CV</a></font></li>
					<li><a href="https://scholar.google.com/citations?user=nDW764QAAAAJ&hl=en" class="ai ai-google-scholar-square ai-2x"></a></li>
						<li><a href="https://twitter.com/rystoli" class="icon fa-twitter fa-2x"><span class="label">Twitter</span></a></li>
						<li><a href="https://github.com/rystoli/" class="icon fa-github fa-2x"><span class="label">Github</span></a></li>
						<li><a href="mailto:rystoli@nyu.edu" class="icon fa-envelope fa-2x"><span class="label">Email</span></a></li>
					</ul>
				</footer>
			</section>

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Main -->
					<div id="main">

						<!-- One -->
							<section id="one">
								<div class="container">
								<a href="./index.html">Back to start</a><br><br>
								
<h3>Research</h3>

Humans can effortlessly infer the expanse of social information about one another. Even from a mere glimpse of a face, we rapidly determine others' thoughts, feelings, personality, race, gender, and age —even political and religous affiliation. Whether accurate or wildly inaccurate, these inferences are reliable, commonplace, and occur within a second. This is especially impressive, as we are often operating in the dark as social perceivers. Mental states, traits, and social category memberships are largely hidden variables, in that we rarely observe them directly. Even then, to infer these things, in most social interaction, we are only given so much information about one another, whether an emotional expression, skin pigmentation, brief words, or gossip. <br><br>

Yet we do this so easily - how so? <br><br>


I study how humans efficiently extract social information, where social perceptions arise as a statistical integration of whatever information is at hand to the perceiver. For instance, to infer someone's intelligence, we may integrate present knowledge of their apparent mood, gender, and our intergroup biases depending on their affiliations. In this fashion my work explores how humans hold rich conceptual maps of how social information correlates in the world (e.g., how intelligence correlates with neuroticism, height, gender, or political party), which are navigated with whatever bottom-up information (e.g., facial cues, gossip) or top-down social factors (e.g., current context, group biases) are present to infer the gamut of social perceptions. In particular, my work bridges behavioral, neuroimaging, and computational methods to develop comprehensive ideas about both the structure and dynamics of these processes. 

<br><br><h4>Trait impressions</h4>

In ongoing research, we find trait impressions based on others' facial appearance are structured by our lay theories of how personality functions in others. For instance, our internal beliefs about how personality traits covary in the world (are kind people often intelligent, creative, neurotic?) may underlie how we infer the expanse of personality traits from a face (e.g., inferring intelligence, creativity, and neuroticism from the kind babyishness of a face; <a href="https://rystoli.github.io/pubs/2018_Stolieretal_PNAS.pdf">Stolier, Hehman, Keller, Walker, & Freeman, 2018 - <i>Proceedings of the National Academy of Sciences</i></a>). This has important implications for how we understand other people, especially as we may make consequential personality impressions (e.g., intelligence) about others based on what little we know about them (e.g., kindness). Furthermore, these trait associations (e.g., how intelligent a kind person is perceived) may shift in certain contexts (e.g., kindness and intelligence judgements become more negatively related towards females, as negative attitudes towards competent females prominent in the United States; Glick & Fiske, 1996 — <i>JPSP</i>). We recently proposed a framework which summarizes these points, providing a parsimonious account in which trait impressions are the integration of bottom-up perception, and many top-down components (<a href="https://rystoli.github.io/pubs/2018_StolierHehmanFreeman_TiCS.pdf">Stolier, Hehman, & Freeman, 2018 - <i>Trends in Cognitive Sciences</i></a>). Following all of this work, we recently proposed and found that lay theories of others' personality shape trait impressions across domains, for instance, also in familiar person knowledge or group stereotypes (<a href="https://psyarxiv.com/5na8m">Stolier, Hehman, & Freeman, under review</a>). In fact, lay personality models explain considerable variance in social perception models across these domains (consistently around 50%), providing a parsimonious theoretical perspective on the common structure seen across models of social perception (e.g., trustworthiness and dominance in face impressions, warmth and competence in group stereotype content).

<br><br><img src="./images/trait_research.jpg" alt="trait_research" style="width:500px">
<br><h5>Perceivers who believe agreeable people are often open-minded infer agreeableness and openness more similarly in faces (<a href="https://psyarxiv.com/d9ha7">Stolier, Hehman, Keller, Walker, & Freeman, submitted</a>).</h5>

<br><br><h4>Social categorizations</h4>

How do we come to categorize others' race, gender, or age? In recent work, we found social categories that share trait stereotypes are perceived more similarly. For instance, male and black categories share 'dominance' trait stereotypes, and thereby facilitate perception of one another and accordingly overlap in neural representational structure (<a href="./pubs/2016_Stolier&Freeman.pdf">Stolier & Freeman, 2016 - <i>Nature Neuroscience</i></a>; <a href="./pubs/2017_BrooksStolierFreeman.pdf">Brooks, Stolier, & Freeman, in press - <i>Social Cognition</i></a>). This suggests perceivers use the presence of other categories in a face, along with knowledge of the traits associated with those categories to perceive any individual social category from a face. This work highlights an important, counter-intuitive route through which trait stereotypes can be activated during perception. In this case, categories perceived from a face (e.g., male) elicit activation of other categories not perceived (e.g., black), which in turn activate stereotypes and evaluations relevant to that other category (for instance, <a href="https://www8.gsb.columbia.edu/newsroom/newsn/2214/gender-and-race-how-overlapping-stereotypes-affect-interracial-dating-leadership-selection-and-athletic-participation">see work on behavioral consequences of this process by Galinsky, Hall, & Cuddy, 2013 — <i>Psychological Science</i></a>).

<br><br><img src="./images/cat_research.png" alt="trait_research" style="width:400px">
<br><h5>People visually represent emotion categories as belonging to specific genders (via reverse correlation task; female cues in happiness, male cues in anger perceptions; figure from a pilot, see <a href="./pubs/2017_BrooksStolierFreeman.pdf">Brooks, Stolier, & Freeman, in press - <i>Social Cognition</i></a>).</h5>

<br><br><h4>Integrative neural networks</h4>

Across these many questions, my work combines behavioral, neuroimaging, and computational methods to build a coherent picture of these processes (<a href="./pubs/2016_Stolier_Freeman_PsychInquiry.pdf">Stolier & Freeman, 2016 - <i>Psychological Inquiry</i></a>). Our work has begun to elucidate how 'initial' social perceptions in visual brain regions are actually influenced by many higher-order processes, from stereotypes (via the orbitofrontal cortex; <a href="./pubs/2016_Stolier&Freeman_NatureNeuro.pdf">Stolier & Freeman, 2016 - <i>Nature Neuroscience</i></a>) to cognitive control (via the dorsal anterior cingulate; <a href="./pubs/2017_StolierFreeman_JNeuro.pdf">Stolier & Freeman, 2017 - <i>Journal of Neuroscience</i></a>). These approaches and insights play a crucial role in our work, both constraining and informing our theoretical perspectives.

<br><br><img src="./images/neuro_research.png" alt="trait_research" style="width:500px">
<br><h5>Neural representations of social categories are similar to the extent those categories share trait stereotypes (<a href="./pubs/2016_Stolier&Freeman_NatureNeuro.pdf">Stolier & Freeman, 2016 - <i>Nature Neuroscience</i></a>).</h5>

<p><h5>Please see the side menu for more information. As well, visit the <a href="http://psych.nyu.edu/freemanlab/index.htm"><u>Freeman Lab site</u></a>.</h5></p>


								</div>
							</section>
				

					</div>

				<!-- Footer -->
					<section id="footer">
						<div class="container">
							<ul class="copyright">
								<li>&copy; Ryan M Stolier. All rights reserved?</li>
							</ul>
						</div>
					</section>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollzer.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/skel.min.js"></script>
			<script src="assets/js/util.js"></script>
			<!--[if lte IE 8]><script src="assets/js/ie/respond.min.js"></script><![endif]-->
			<script src="assets/js/main.js"></script>

	</body>
</html>